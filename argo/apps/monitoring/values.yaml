# Temporarily disable everything other than grafana
domain: REPLACE_ME
cluster_name: REPLACE_ME

grafana:
  enabled: true
  ingress:
    enabled: true
    hosts:
      - grafana.REPLACE_ME
    ingressClassName: internal-shared # Auth using Dex, but we want to share the instance
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.REPLACE_ME
  grafana.ini:
    server:
      domain: https://REPLACE_ME
      root_url: REPLACE_ME
    users:
      allow_sign_up: false
      auto_assign_org: true
      # TODO: Reduce it once we have more users
      auto_assign_org_role: Admin
      allow_assign_grafana_admin: true
    auth:
      oauth_auto_login: true
    auth.generic_oauth:
      name: Dex
      enabled: true
      client_id: $__file{/etc/secrets/dex_oauth/client_id}
      client_secret: $__file{/etc/secrets/dex_oauth/client_secret}
      scopes: openid email profile groups offline_access
      auth_url: https://login.REPLACE_ME/auth
      token_url: https://login.REPLACE_ME/token
      api_url: https://login.REPLACE_ME/userinfo
    auth.basic:
      enabled: false
  extraSecretMounts:
    - name: dex-oauth
      secretName: grafana-dex-oauth-secret
      defaultMode: 0440
      mountPath: /etc/secrets/dex_oauth
      readOnly: true
  sidecar:
    dashboards:
      enabled: true
      labelValue: "true"
    datasources:
      enabled: true
      labelValue: "true"
  # Disable as its not currently working
  plugins:
    - grafana-oncall-app

mimir:
  enabled: true
  ingress:
    enabled: true
    ingressClassName: internal-login
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
  # Low traffic home clustes, no need for more than 1 replica and there are probably no zones.
  # downtime on upgrade is tolerated
  ingester:
    replicas: 2
    zoneAwareReplication:
      enabled: false
  querier:
    replicas: 2

oncall:
  enabled: false
  base_url: oncall.REPLACE_ME
  externalGrafana:
    url: https://grafana.REPLACE_ME
  ingress:
    enabled: true
    className: internal
    annotations: null # Clear the annotations because the chart is stupid
    tls:
      - secretName: oncall-tls # Follow our own conventions
        hosts:
          - "{{ $.Values.oncall.base_url }}"

  database:
    type: postgresql

  migrate:
    useHook: true
    annotations:
      argocd.argoproj.io/hook: PostSync
      argocd.argoproj.io/hook-delete-policy: HookSucceeded

  # Copied from the config, not sure will it work
  postgresql:
    enabled: true # Must be set to true for bundled Postgres
    auth:
      existingSecret: ""
      secretKeys:
        adminPasswordKey: ""
        userPasswordKey: "" # Not needed
        replicationPasswordKey: "" # Not needed with disabled replication
      # Secret name: `<release>-postgresql`
      postgresPassword: "" # password for admin user postgres. As non-admin user is not created, only this one is relevant.
      password: "" # Not needed
      replicationPassword: "" # Not needed with disabled replication

  # copied from config, not sure if needed
  rabbitmq:
    enabled: true
    auth:
      existingPasswordSecret: "" # Must contain `rabbitmq-password` key
      existingErlangSecret: "" # Must contain `rabbitmq-erlang-cookie` key
      # Secret name: `<release>-rabbitmq`
      password: ""
      erlangCookie: ""

  # copied from config, not sure if needed
  redis:
    enabled: true
    auth:
      existingSecret: ""
      existingSecretPasswordKey: ""
      # Secret name: `<release>-redis`
      password: ""

  # Disable the default stuff that is not needed
  ingress-nginx:
    enabled: false
  cert-manager:
    enabled: false
  grafana:
    enabled: false
  prometheus:
    enabled: false
  mariadb:
    enabled: false

alloy:
  enabled: true
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    ingressClassName: internal-shared
  alloy:
    configMap:
      content: |
        logging {
          level = "info"
          format = "json"
        }

        discovery.kubernetes "in_cluster" {
          role = "pod"
        }

        prometheus.scrape "in_cluster" {
          targets    = discovery.kubernetes.in_cluster.targets
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.operator.servicemonitors "services" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.remote_write "mimir" {
          endpoint {
            url = "http://monitoring-mimir-nginx.monitoring:80/api/v1/push"
            headers = {
              "X-Scope-OrgID" = env("CLUSTER_NAME"),
            }
          }
        }

tempo:
  enabled: false
loki:
  enabled: false
