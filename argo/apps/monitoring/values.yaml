# Temporarily disable everything other than grafana
domain: REPLACE_ME
cluster_name: REPLACE_ME

grafana:
  enabled: true
  ingress:
    enabled: true
    hosts:
      - grafana.REPLACE_ME
    ingressClassName: internal-shared # Auth using Dex, but we want to share the instance
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.REPLACE_ME
  grafana.ini:
    security:
      disable_initial_admin_creation: true # Prevent helm re-create the admin user
    server:
      domain: https://REPLACE_ME
      root_url: REPLACE_ME
    users:
      allow_sign_up: false
      auto_assign_org: true
      # TODO: Reduce it once we have more users
      auto_assign_org_role: Admin
      allow_assign_grafana_admin: true
    auth:
      oauth_auto_login: true
    auth.generic_oauth:
      name: Dex
      enabled: true
      client_id: $__file{/etc/secrets/dex_oauth/client_id}
      client_secret: $__file{/etc/secrets/dex_oauth/client_secret}
      scopes: openid email profile groups offline_access
      auth_url: https://login.REPLACE_ME/auth
      token_url: https://login.REPLACE_ME/token
      api_url: https://login.REPLACE_ME/userinfo
    auth.basic:
      enabled: false
  extraSecretMounts:
    - name: dex-oauth
      secretName: grafana-dex-oauth-secret
      defaultMode: 0440
      mountPath: /etc/secrets/dex_oauth
      readOnly: true
  sidecar:
    dashboards:
      enabled: true
      labelValue: "true"
    datasources:
      enabled: true
      labelValue: "true"
  adminPassword: "UNUSED" # We only allow login via Dex
  # Disable as its not currently working
  plugins:
    - grafana-oncall-app

mimir:
  enabled: true
  ingress:
    enabled: true
    ingressClassName: internal-login
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
  # Reduce the replicas to 2 each as they should be fine in a home cluster
  distributor:
    replicas: 2
  ingester:
    replicas: 2
    zoneAwareReplication:
      enabled: false
  querier:
    replicas: 2
  store_gateway:
    zoneAwareReplication:
      enabled: false

  memberlist:
    cluster_label: mimir
    cluster_label_verification_disabled: true

  limits:
    native_histograms_ingestion_enabled: true
    out_of_order_ingestion_enabled: 24h # Increase the limit in case something goes down
    cardinality_analysis_enabled: true
    compactor_blocks_retention_period: 720h # One month retention

  metaMonitoring:
    dashboards:
      enabled: true
      labels:
        grafana_dashboard: "true"
    serviceMonitor:
      enabled: true
      clusterLabel: EXTERNAL
    prometheusRule:
      enable: true
      mimirAlerts: true
      mimirRules: true

oncall:
  enabled: false
  base_url: oncall.REPLACE_ME
  externalGrafana:
    url: https://grafana.REPLACE_ME
  ingress:
    enabled: true
    className: internal
    annotations: null # Clear the annotations because the chart is stupid
    tls:
      - secretName: oncall-tls # Follow our own conventions
        hosts:
          - "{{ $.Values.oncall.base_url }}"

  database:
    type: postgresql

  migrate:
    useHook: true
    annotations:
      argocd.argoproj.io/hook: PostSync
      argocd.argoproj.io/hook-delete-policy: HookSucceeded

  # Copied from the config, not sure will it work
  postgresql:
    enabled: true # Must be set to true for bundled Postgres
    auth:
      existingSecret: ""
      secretKeys:
        adminPasswordKey: ""
        userPasswordKey: "" # Not needed
        replicationPasswordKey: "" # Not needed with disabled replication
      # Secret name: `<release>-postgresql`
      postgresPassword: "" # password for admin user postgres. As non-admin user is not created, only this one is relevant.
      password: "" # Not needed
      replicationPassword: "" # Not needed with disabled replication

  # copied from config, not sure if needed
  rabbitmq:
    enabled: true
    auth:
      existingPasswordSecret: "" # Must contain `rabbitmq-password` key
      existingErlangSecret: "" # Must contain `rabbitmq-erlang-cookie` key
      # Secret name: `<release>-rabbitmq`
      password: ""
      erlangCookie: ""

  # copied from config, not sure if needed
  redis:
    enabled: true
    auth:
      existingSecret: ""
      existingSecretPasswordKey: ""
      # Secret name: `<release>-redis`
      password: ""

  # Disable the default stuff that is not needed
  ingress-nginx:
    enabled: false
  cert-manager:
    enabled: false
  grafana:
    enabled: false
  prometheus:
    enabled: false
  mariadb:
    enabled: false

alloy:
  enabled: true
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    ingressClassName: internal-shared
    faroPort: 12345 # Fake faro so we have access to the UI
    extraPaths:
      - path: /faro
        pathType: Prefix
        backend:
          service:
            name: monitoring-alloy
            port:
              number: 12347
  alloy:
    stabilityLevel: experimental
    configMap:
      content: |
        logging {
          level = "info"
          format = "json"
        }

        livedebugging {
          enabled = true
        }

        prometheus.operator.servicemonitors "services" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.remote_write "mimir" {
          endpoint {
            url = "http://monitoring-mimir-nginx.monitoring:80/api/v1/push"
            headers = {
              "X-Scope-OrgID" = env("CLUSTER_NAME"),
            }
          }
        }

        // Logs

        discovery.kubernetes "pod" {
          role = "pod"
        }

        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pod.targets

          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action        = "replace"
            target_label  = "namespace"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action        = "replace"
            target_label  = "pod"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "container"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action        = "replace"
            target_label  = "app"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "job"
            separator     = "/"
            replacement   = "$1"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "__path__"
            separator     = "/"
            replacement   = "/var/log/pods/*$1/*.log"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_container_id"]
            action        = "replace"
            target_label  = "container_runtime"
            regex         = "^(\\S+):\\/\\/.+$"
            replacement   = "$1"
          }
        }

        loki.source.kubernetes "in_cluster" {
          targets = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }

        loki.process "pod_logs" {
          // stage.cri {}

          forward_to = [loki.write.loki.receiver]
        }

        loki.write "loki" {
          endpoint {
            url = "http://monitoring-loki-gateway.monitoring.svc:80"
          }
        }

loki:
  global:
    clusterDomain: cluster.local
    # try override the DNS server settings
    dnsService: coredns

  enabled: true
  loki:
    commonConfig:
      replication_factor: 1
    schemaConfig:
      configs:
        - from: 2024-04-01
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    ingester:
      chunk_encoding: snappy
    tracing:
      enabled: true
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 2

  gateway:
    ingress:
      enabled: true
    nginxConfig:
      resolver: "127.0.0.1:8053 valid=5s ipv6=off"
    extraContainers:
      # resolves issues with DNS resolution of services
      # see https://gist.github.com/joemiller/68ab3f7a7a08e4a9d5ad5d023cb14fc2
      - name: dnsmasq
        image: "janeczku/go-dnsmasq:release-1.0.7"
        imagePullPolicy: IfNotPresent
        args:
          - --listen
          - "127.0.0.1:8053"
          - --hostsfile=/etc/hosts
          - --verbose

  deploymentMode: SingleBinary
  singleBinary:
    replicas: 1
    resources:
      requests:
        cpu: 2
        memory: 2Gi
    extraEnv:
      # Keep a little bit lower than memory limits
      - name: GOMEMLIMIT
        value: 1850MiB

  memberlist:
    cluster_label: loki
    cluster_label_verification_disabled: true

  chunksCache:
    # default is 500MB, with limited memory keep this smaller
    writebackSizeLimit: 10MB

  # Enable minio for storage
  minio:
    enabled: true

  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

tempo:
  enabled: false

prometheus-node-exporter:
  enabled: true
  prometheus:
    monitor:
      enabled: true
