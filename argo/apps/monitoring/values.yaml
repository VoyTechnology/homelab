domain: REPLACE_ME
cluster_name: REPLACE_ME

####################################################################################################
# Grafana
####################################################################################################


grafana:
  enabled: true
  ingress:
    enabled: true
    hosts:
      - grafana.REPLACE_ME
    ingressClassName: internal-shared # Auth using Dex, but we want to share the instance
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.REPLACE_ME
  grafana.ini:
    security:
      disable_initial_admin_creation: true # Prevent helm re-create the admin user
    server:
      domain: https://REPLACE_ME
      root_url: REPLACE_ME
    users:
      allow_sign_up: false
      auto_assign_org: true
      # TODO: Reduce it once we have more users
      auto_assign_org_role: Admin
      allow_assign_grafana_admin: true
    auth:
      oauth_auto_login: true
    auth.generic_oauth:
      name: Dex
      enabled: true
      client_id: $__file{/etc/secrets/dex_oauth/client_id}
      client_secret: $__file{/etc/secrets/dex_oauth/client_secret}
      scopes: openid email profile groups offline_access
      auth_url: https://login.REPLACE_ME/auth
      token_url: https://login.REPLACE_ME/token
      api_url: https://login.REPLACE_ME/userinfo
    auth.basic:
      enabled: false
  extraSecretMounts:
    - name: dex-oauth
      secretName: grafana-dex-oauth-secret
      defaultMode: 0440
      mountPath: /etc/secrets/dex_oauth
      readOnly: true
  sidecar:
    dashboards:
      enabled: true
      labelValue: "true"
      searchNamespace: ALL # Search all namespaces for dashboards
    datasources:
      enabled: true
      labelValue: "true"
      searchNamespace: ALL # Search all namespaces for dashboards
  adminPassword: "UNUSED" # We only allow login via Dex
  plugins: []
  # Check are the DNS issues gone and everything works
  # dnsConfig:
  #   nameservers:
  #     - 127.0.0.1:8053
  # extraContainers:
  #   # resolves issues with DNS resolution of services
  #   # see https://gist.github.com/joemiller/68ab3f7a7a08e4a9d5ad5d023cb14fc2
  #   - name: dnsmasq
  #     image: "janeczku/go-dnsmasq:release-1.0.7"
  #     imagePullPolicy: IfNotPresent
  #     args:
  #       - --listen
  #       - "127.0.0.1:8053"
  #       - --hostsfile=/etc/hosts
  #       - --verbose

####################################################################################################
# Alloy
####################################################################################################

alloy:
  enabled: true
  controller:
    type: statefulset # Needed for clustering
    replicas: 2
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
    ingressClassName: internal-shared
    faroPort: 12345 # Fake faro so we have access to the UI
    extraPaths:
      - path: /faro
        pathType: Prefix
        backend:
          service:
            name: monitoring-alloy
            port:
              number: 12347
  alloy:
    clustering:
      enabled: false
    stabilityLevel: experimental
    configMap:
      content: |
        logging {
          level = "info"
          format = "json"
        }

        livedebugging {
          enabled = true
        }

        // Shared config
        discovery.kubernetes "node" {
          role = "node"
        }

        ////////////////////////////////////////////////////////////////////////
        // Metrics
        ////////////////////////////////////////////////////////////////////////

        // We use the servicemonitor to allow services their own control
        // over how they are scraped
        prometheus.operator.servicemonitors "services" {
          clustering {
            enabled = false
          }

          // forward_to = [prometheus.relabel.services.receiver]
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.exporter.cadvisor "cadvisor" {}

        prometheus.scrape "cadvisor" {
          targets = prometheus.exporter.cadvisor.cadvisor.targets
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.scrape "node" {
          targets = discovery.kubernetes.node.targets
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        // prometheus.relabel "services" {
        //   forward_to = [prometheus.remote_write.mimir.receiver]
        //
        //   // In clustering mode it seems like the instance is not set correctly
        //   // https://github.com/grafana/alloy/issues/1009
        //   rule {
        //     action = "labe"
        //   }
        // }

        prometheus.remote_write "mimir" {
          endpoint {
            url = "http://metrics-mimir-nginx.metrics/api/v1/push"
            headers = {
              "X-Scope-OrgID" = env("CLUSTER_NAME"),
            }
          }
        }

        ////////////////////////////////////////////////////////////////////////
        // Logs
        ////////////////////////////////////////////////////////////////////////

        discovery.kubernetes "pod" {
          role = "pod"
        }

        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pod.targets

          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action        = "replace"
            target_label  = "namespace"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action        = "replace"
            target_label  = "pod"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "container"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action        = "replace"
            target_label  = "app"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "job"
            separator     = "/"
            replacement   = "$1"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "__path__"
            separator     = "/"
            replacement   = "/var/log/pods/*$1/*.log"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_container_id"]
            action        = "replace"
            target_label  = "container_runtime"
            regex         = "^(\\S+):\\/\\/.+$"
            replacement   = "$1"
          }
        }

        loki.source.kubernetes "in_cluster" {
          targets = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }

        loki.process "pod_logs" {
          stage.cri {}

          forward_to = [loki.write.loki.receiver]
        }

        loki.write "loki" {
          endpoint {
            url = "http://logs-loki-gateway.logs/loki/api/v1/push"
            tenant_id = env("CLUSTER_NAME")
          }
        }
####################################################################################################
# Tempo
####################################################################################################

tempo:
  enabled: false

####################################################################################################
# Node Exporter
####################################################################################################

prometheus-node-exporter:
  enabled: true
  prometheus:
    monitor:
      enabled: true
